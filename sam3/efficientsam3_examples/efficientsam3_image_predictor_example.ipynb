{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Image segmentation with EfficientSAM3\n",
        "\n",
        "This notebook demonstrates how to use EfficientSAM3 for image segmentation with text or visual prompts. It covers the following capabilities:\n",
        "\n",
        "- **Text prompts**: Using natural language descriptions to segment objects (e.g., \"person\", \"face\")\n",
        "- **Box prompts**: Using bounding boxes as exemplar visual prompts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a target=\"_blank\" href=\"https://colab.research.google.com/github/SimonZeng7108/efficientsam3/blob/main/efficientsam3_examples/efficientsam3_image_predictor_example.ipynb\">\n",
        "#   <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "# </a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "using_colab = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if using_colab:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(\"PyTorch version:\", torch.__version__)\n",
        "    print(\"Torchvision version:\", torchvision.__version__)\n",
        "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install opencv-python matplotlib scikit-learn\n",
        "    !{sys.executable} -m pip install 'git+https://github.com/SimonZeng7108/efficientsam3.git'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import sam3\n",
        "from PIL import Image\n",
        "from sam3.model.box_ops import box_xywh_to_cxcywh\n",
        "from sam3.model.sam3_image_processor import Sam3Processor\n",
        "from sam3.visualization_utils import draw_box_on_image, normalize_bbox, plot_results\n",
        "\n",
        "sam3_root = os.path.join(os.path.dirname(sam3.__file__), \"..\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# turn on tfloat32 for Ampere GPUs\n",
        "# https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# use bfloat16 for the entire notebook\n",
        "torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sam3.model_builder import build_efficientsam3_image_model\n",
        "\n",
        "bpe_path = f\"{sam3_root}/assets/bpe_simple_vocab_16e6.txt.gz\"\n",
        "model = build_efficientsam3_image_model(\n",
        "    bpe_path=bpe_path,\n",
        "    checkpoint_path=None,  # Set to your EfficientSAM3 checkpoint path\n",
        "    load_from_HF=False,  # Set to True if loading from HuggingFace\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_path = f\"{sam3_root}/assets/images/test_image.jpg\"\n",
        "image = Image.open(image_path)\n",
        "width, height = image.size\n",
        "processor = Sam3Processor(model, confidence_threshold=0.5)\n",
        "inference_state = processor.set_image(image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "processor.reset_all_prompts(inference_state)\n",
        "inference_state = processor.set_text_prompt(state=inference_state, prompt=\"shoe\")\n",
        "\n",
        "img0 = Image.open(image_path)\n",
        "plot_results(img0, inference_state)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visual prompt: a single bounding box\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Here the box is in  (x,y,w,h) format, where (x,y) is the top left corner.\n",
        "box_input_xywh = torch.tensor([480.0, 290.0, 110.0, 360.0]).view(-1, 4)\n",
        "box_input_cxcywh = box_xywh_to_cxcywh(box_input_xywh)\n",
        "\n",
        "norm_box_cxcywh = normalize_bbox(box_input_cxcywh, width, height).flatten().tolist()\n",
        "print(\"Normalized box input:\", norm_box_cxcywh)\n",
        "\n",
        "processor.reset_all_prompts(inference_state)\n",
        "inference_state = processor.add_geometric_prompt(\n",
        "    state=inference_state, box=norm_box_cxcywh, label=True\n",
        ")\n",
        "\n",
        "img0 = Image.open(image_path)\n",
        "image_with_box = draw_box_on_image(img0, box_input_xywh.flatten().tolist())\n",
        "plt.imshow(image_with_box)\n",
        "plt.axis(\"off\")  # Hide the axis\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_results(img0, inference_state)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visual prompt: multi-box prompting (with positive and negative boxes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "box_input_xywh = [[480.0, 290.0, 110.0, 360.0], [370.0, 280.0, 115.0, 375.0]]\n",
        "box_input_cxcywh = box_xywh_to_cxcywh(torch.tensor(box_input_xywh).view(-1,4))\n",
        "norm_boxes_cxcywh = normalize_bbox(box_input_cxcywh, width, height).tolist()\n",
        "\n",
        "box_labels = [True, False]\n",
        "\n",
        "processor.reset_all_prompts(inference_state)\n",
        "\n",
        "for box, label in zip(norm_boxes_cxcywh, box_labels):\n",
        "    inference_state = processor.add_geometric_prompt(\n",
        "        state=inference_state, box=box, label=label\n",
        "    )\n",
        "\n",
        "img0 = Image.open(image_path)\n",
        "image_with_box = img0\n",
        "for i in range(len(box_input_xywh)):\n",
        "    if box_labels[i] == 1:\n",
        "        color = (0, 255, 0)\n",
        "    else:\n",
        "        color = (255, 0, 0)\n",
        "    image_with_box = draw_box_on_image(image_with_box, box_input_xywh[i], color)\n",
        "plt.imshow(image_with_box)\n",
        "plt.axis(\"off\")  # Hide the axis\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_results(img0, inference_state)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
