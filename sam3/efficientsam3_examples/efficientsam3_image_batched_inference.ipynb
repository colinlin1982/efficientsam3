{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a target=\"_blank\" href=\"https://colab.research.google.com/github/SimonZeng7108/efficientsam3/blob/main/efficientsam3_examples/efficientsam3_image_batched_inference.ipynb\">\n",
        "#   <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "# </a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "using_colab = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if using_colab:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(\"PyTorch version:\", torch.__version__)\n",
        "    print(\"Torchvision version:\", torchvision.__version__)\n",
        "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install opencv-python matplotlib scikit-learn\n",
        "    !{sys.executable} -m pip install 'git+https://github.com/SimonZeng7108/efficientsam3.git'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import sam3\n",
        "from sam3.train.data.collator import collate_fn_api as collate\n",
        "from sam3.model.utils.misc import copy_data_to_device\n",
        "import os\n",
        "sam3_root = os.path.join(os.path.dirname(sam3.__file__), \"..\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "# turn on tfloat32 for Ampere GPUs\n",
        "# https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# use bfloat16 for the entire notebook. If your card doesn't support it, try float16 instead\n",
        "torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "\n",
        "# inference mode for the whole notebook. Disable if you need gradients\n",
        "torch.inference_mode().__enter__()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Utilities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section contains simple utilities to plot masks and bounding masks on top of an image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(f\"{sam3_root}/examples\")\n",
        "\n",
        "from sam3.visualization_utils import plot_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batching\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section contains some utility functions to create datapoints. They are optional, but give some good indication on how they should be created\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sam3.train.data.sam3_image_dataset import InferenceMetadata, FindQueryLoaded, Image as SAMImage, Datapoint\n",
        "from typing import List\n",
        "\n",
        "GLOBAL_COUNTER = 1\n",
        "def create_empty_datapoint():\n",
        "    \"\"\" A datapoint is a single image on which we can apply several queries at once. \"\"\"\n",
        "    return Datapoint(find_queries=[], images=[])\n",
        "\n",
        "def set_image(datapoint, pil_image):\n",
        "    \"\"\" Add the image to be processed to the datapoint \"\"\"\n",
        "    w,h = pil_image.size\n",
        "    datapoint.images = [SAMImage(data=pil_image, objects=[], size=[h,w])]\n",
        "\n",
        "def add_text_prompt(datapoint, text_query):\n",
        "    \"\"\" Add a text query to the datapoint \"\"\"\n",
        "\n",
        "    global GLOBAL_COUNTER\n",
        "    # in this function, we require that the image is already set.\n",
        "    # that's because we'll get its size to figure out what dimension to resize masks and boxes\n",
        "    # In practice you're free to set any size you want, just edit the rest of the function\n",
        "    assert len(datapoint.images) == 1, \"please set the image first\"\n",
        "\n",
        "    w, h = datapoint.images[0].size\n",
        "    datapoint.find_queries.append(\n",
        "        FindQueryLoaded(\n",
        "            query_text=text_query,\n",
        "            image_id=0,\n",
        "            object_ids_output=[], # unused for inference\n",
        "            is_exhaustive=True, # unused for inference\n",
        "            query_processing_order=0,\n",
        "            inference_metadata=InferenceMetadata(\n",
        "                coco_image_id=GLOBAL_COUNTER,\n",
        "                original_image_id=GLOBAL_COUNTER,\n",
        "                original_category_id=1,\n",
        "                original_size=[w, h],\n",
        "                object_id=0,\n",
        "                frame_index=0,\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    GLOBAL_COUNTER += 1\n",
        "    return GLOBAL_COUNTER - 1\n",
        "\n",
        "def add_visual_prompt(datapoint, boxes:List[List[float]], labels:List[bool], text_prompt=\"visual\"):\n",
        "    \"\"\" Add a visual query to the datapoint.\n",
        "    The bboxes are expected in XYXY format (top left and bottom right corners)\n",
        "    For each bbox, we expect a label (true or false). The model tries to find boxes that ressemble the positive ones while avoiding the negative ones\n",
        "    We can also give a text_prompt as an additional hint. It's not mandatory, leave it to \"visual\" if you want the model to solely rely on the boxes.\n",
        "\n",
        "    Note that the model expects the prompt to be consistent. If the text reads \"elephant\" but the provided boxe points to a dog, the results will be undefined.\n",
        "    \"\"\"\n",
        "\n",
        "    global GLOBAL_COUNTER\n",
        "    # in this function, we require that the image is already set.\n",
        "    # that's because we'll get its size to figure out what dimension to resize masks and boxes\n",
        "    # In practice you're free to set any size you want, just edit the rest of the function\n",
        "    assert len(datapoint.images) == 1, \"please set the image first\"\n",
        "    assert len(boxes) > 0, \"please provide at least one box\"\n",
        "    assert len(boxes) == len(labels), f\"Expecting one label per box. Found {len(boxes)} boxes but {len(labels)} labels\"\n",
        "    for b in boxes:\n",
        "        assert len(b) == 4, f\"Boxes must have 4 coordinates, found {len(b)}\"\n",
        "\n",
        "    labels = torch.tensor(labels, dtype=torch.bool).view(-1)\n",
        "    if not labels.any().item() and text_prompt==\"visual\":\n",
        "        print(\"Warning: you provided no positive box, nor any text prompt. The prompt is ambiguous and the results will be undefined\")\n",
        "    w, h = datapoint.images[0].size\n",
        "    datapoint.find_queries.append(\n",
        "        FindQueryLoaded(\n",
        "            query_text=text_prompt,\n",
        "            image_id=0,\n",
        "            object_ids_output=[], # unused for inference\n",
        "            is_exhaustive=True, # unused for inference\n",
        "            query_processing_order=0,\n",
        "            input_bbox=torch.tensor(boxes, dtype=torch.float).view(-1,4),\n",
        "            input_bbox_label=labels,\n",
        "            inference_metadata=InferenceMetadata(\n",
        "                coco_image_id=GLOBAL_COUNTER,\n",
        "                original_image_id=GLOBAL_COUNTER,\n",
        "                original_category_id=1,\n",
        "                original_size=[w, h],\n",
        "                object_id=0,\n",
        "                frame_index=0,\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "    GLOBAL_COUNTER += 1\n",
        "    return GLOBAL_COUNTER - 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we load our model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sam3.model_builder import build_efficientsam3_image_model\n",
        "\n",
        "bpe_path = f\"{sam3_root}/assets/bpe_simple_vocab_16e6.txt.gz\"\n",
        "model = build_efficientsam3_image_model(\n",
        "    bpe_path=bpe_path,\n",
        "    checkpoint_path=None,  # Set to your EfficientSAM3 checkpoint path\n",
        "    load_from_HF=False,  # Set to True if loading from HuggingFace\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then our validation transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sam3.train.transforms.basic_for_api import ComposeAPI, RandomResizeAPI, ToTensorAPI, NormalizeAPI\n",
        "\n",
        "from sam3.model.position_encoding import PositionEmbeddingSine\n",
        "transform = ComposeAPI(\n",
        "    transforms=[\n",
        "        RandomResizeAPI(sizes=1008, max_size=1008, square=True, consistent_transform=False),\n",
        "        ToTensorAPI(),\n",
        "        NormalizeAPI(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And finally our postprocessor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sam3.eval.postprocessors import PostProcessImage\n",
        "postprocessor = PostProcessImage(\n",
        "    max_dets_per_img=-1,       # if this number is positive, the processor will return topk. For this demo we instead limit by confidence, see below\n",
        "    iou_type=\"segm\",           # we want masks\n",
        "    use_original_sizes_box=True,   # our boxes should be resized to the image size\n",
        "    use_original_sizes_mask=True,   # our masks should be resized to the image size\n",
        "    convert_mask_to_rle=False, # the postprocessor supports efficient conversion to RLE format. In this demo we prefer the binary format for easy plotting\n",
        "    detection_threshold=0.5,   # Only return confident detections\n",
        "    to_cpu=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For inference, we proceed as follows:\n",
        "- Create each datapoint one by one, using the functions above. Each query that we make will give us a unique id, which is then used after post-processing to retrieve the results\n",
        "- Each datapoint must be transformed according to are pre-processing transforms (basically resize to 1008x1008, normalize)\n",
        "- We then collate all datapoints into a batch and forward it to the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image 1, we'll use two text prompts\n",
        "\n",
        "img1 = Image.open(BytesIO(requests.get(\"http://images.cocodataset.org/val2017/000000077595.jpg\").content))\n",
        "datapoint1 = create_empty_datapoint()\n",
        "set_image(datapoint1, img1)\n",
        "id1 = add_text_prompt(datapoint1, \"cat\")\n",
        "id2 = add_text_prompt(datapoint1, \"laptop\")\n",
        "\n",
        "datapoint1 = transform(datapoint1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image 2, one text prompt, some visual prompt\n",
        "img2 = Image.open(BytesIO(requests.get(\"http://images.cocodataset.org/val2017/000000136466.jpg\").content))\n",
        "\n",
        "# img2 = Image.open(f\"{sam3_root}/assets/images/test_image.jpg\")\n",
        "datapoint2 = create_empty_datapoint()\n",
        "set_image(datapoint2, img2)\n",
        "id3 = add_text_prompt(datapoint2, \"pot\")\n",
        "# we trying to find the dials on the oven. Let's give a positive box\n",
        "id4 = add_visual_prompt(datapoint2, boxes=[[ 59, 144,  76, 163]], labels=[True])\n",
        "# Let's also get the oven start/stop button\n",
        "id5 = add_visual_prompt(datapoint2, boxes=[[ 59, 144,  76, 163],[ 87, 148, 104, 159]], labels=[True, True])\n",
        "# Next, let's try to find the pot handles. With the text prompt \"handle\" (vague on purpose), the model also finds the oven's handles\n",
        "# We could make the text query more precise (try it!) but for this example, we instead want to leverage a negative prompt\n",
        "# First, let's see what happens with just the text prompt\n",
        "id6 = add_text_prompt(datapoint2, \"handle\")\n",
        "# now the same but adding the negative prompt\n",
        "id7 = add_visual_prompt(datapoint2, boxes=[[ 40, 183, 318, 204]], labels=[False], text_prompt=\"handle\")\n",
        "\n",
        "datapoint2 = transform(datapoint2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collate then move to cuda\n",
        "batch = collate([datapoint1, datapoint2], dict_key=\"dummy\")[\"dummy\"]\n",
        "batch = copy_data_to_device(batch, torch.device(\"cuda\"), non_blocking=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Forward. Note that the first forward will be very slow due to compilation\n",
        "output = model(batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "processed_results = postprocessor.process_results(output, batch.find_metadatas)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plotting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_results(img1, processed_results[id1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_results(img1, processed_results[id2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# this is the prompt \"pot\"\n",
        "plot_results(img2, processed_results[id3])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is the result of the visual prompt. We prompted for the left-most dial, the model correctly found all of them.\n",
        "plot_results(img2, processed_results[id4])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is the same as above, but we also added a prompt for the on/off switch\n",
        "plot_results(img2, processed_results[id5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# this is the prompt \"handle\". Notice the oven handles that we want to remove\n",
        "plot_results(img2, processed_results[id6])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This time we added the negative prompt for the oven handle and the unwanted boxes are gone\n",
        "plot_results(img2, processed_results[id7])\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
