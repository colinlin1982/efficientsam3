#!/usr/bin/env python3
"""
EfficientSAM3 Image Predictor Example Script
converted from efficientsam3_image_predictor_example.ipynb

"""

import os
import sys
import torch
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import argparse

# Ensure sam3 is in python path
current_dir = os.path.dirname(os.path.abspath(__file__))
# This file lives at: <project_root>/sam3/efficientsam3_examples/efficientsam3_image_predictor_example.py
# `project_root` is the parent of the `sam3/` python package folder.
sam3_pkg_dir = os.path.abspath(os.path.join(current_dir, ".."))  # .../<project_root>/sam3
project_root = os.path.abspath(os.path.join(sam3_pkg_dir, ".."))  # .../<project_root>
if project_root not in sys.path:
    sys.path.append(project_root)

import sam3
from sam3.model_builder import build_sam3_image_model, _create_student_text_encoder, _load_checkpoint
from sam3.model.sam3_image_processor import Sam3Processor
from sam3.visualization_utils import draw_box_on_image, normalize_bbox, plot_results
from sam3.model.box_ops import box_xywh_to_cxcywh

def parse_args():
    parser = argparse.ArgumentParser(description="EfficientSAM3 Image Predictor Example")
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.1,
        help="Confidence threshold for mask presence (default: 0.1)"
    )
    parser.add_argument(
        "--checkpoint", 
        type=str,
        default=None,
        help="Path to the checkpoint file"
    )
    return parser.parse_args()

def main():
    args = parse_args()
    print("Setting up device...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    if device == "cuda":
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True

    # 1. Setup Paths
    # We resolve relative to sam3 package location
    # In the notebook: f"{sam3_root}/assets/bpe_simple_vocab_16e6.txt.gz"
    # sam3_root is defined as parent of sam3.__file__
    
    # Assets live under <project_root>/sam3/assets
    bpe_path = os.path.join(project_root, "sam3", "assets", "bpe_simple_vocab_16e6.txt.gz")
    image_path = os.path.join(project_root, "sam3", "assets", "dog_person.jpeg")
    
    # Checkpoint path
    # This example defaults to a *hybrid* checkpoint:
    # - SAM3 ViT vision encoder (unchanged)
    # - MobileCLIP-S0 student text encoder (replaces teacher text encoder)
    if args.checkpoint:
        checkpoint_path = args.checkpoint
    else:
        checkpoint_path = os.path.join(
            project_root, "output", "efficient_sam3_image_encoder_mobileclip_s0.pth"
        )

    if not os.path.exists(image_path):
        print(f"Error: Image not found at {image_path}")
        # print cwd for debugging
        print(f"CWD: {os.getcwd()}")
        return
    if not os.path.exists(checkpoint_path):
        print(f"Error: Checkpoint not found at {checkpoint_path}")
        return

    # 2. Load Model
    print("Loading SAM3 (ViT vision) + MobileCLIP-S0 (student text) model...")
    # Build a standard SAM3 image model (ViT vision backbone).
    # Then swap in a MobileCLIP student text encoder so the checkpoint's
    # `detector.backbone.language_backbone.*` keys load into the correct module.
    model = build_sam3_image_model(
        bpe_path=bpe_path,
        checkpoint_path=None,  # load after swapping text encoder
        load_from_HF=False,
        enable_segmentation=True,
        enable_inst_interactivity=False,
        compile=False,
        device=device,
    )
    model.backbone.language_backbone = _create_student_text_encoder(
        bpe_path=bpe_path,
        backbone_type="MobileCLIP-S0",
    ).to(device)
    _load_checkpoint(model, checkpoint_path)
    model.to(device)
    model.eval()

    # 3. Load Image & Initialize Processor
    print(f"Loading image: {image_path}")
    
    image_pil = Image.open(image_path).convert("RGB")
    width, height = image_pil.size
    print(f"Image Size: {width}x{height}")
    
    dtype_context = torch.autocast("cuda", dtype=torch.bfloat16) if device == "cuda" else torch.no_grad()
    
    with dtype_context:
        print(f"Using confidence threshold: {args.threshold}")
        processor = Sam3Processor(model, confidence_threshold=args.threshold) 
        inference_state = processor.set_image(image_pil)

        # Output directory
        output_dir = os.path.join(project_root, "output")
        os.makedirs(output_dir, exist_ok=True)

        # ---------------------------------------------------------
        # Text Prompt Examples: "dog" and "person"
        # ---------------------------------------------------------
        print("\n--- Text Prompt 'dog' ---")
        processor.reset_all_prompts(inference_state)
        inference_state = processor.set_text_prompt(state=inference_state, prompt="dog")
        
        # Visualize
        output_path_dog = os.path.join(output_dir, "dog_person_example_dog.png")
        plot_results(image_pil, inference_state)
        plt.axis('off')
        plt.savefig(output_path_dog, bbox_inches='tight', pad_inches=0)
        plt.close()
        print(f"Saved result to {output_path_dog}")

        print("\n--- Text Prompt 'person' ---")
        processor.reset_all_prompts(inference_state)
        inference_state = processor.set_text_prompt(state=inference_state, prompt="person")
        
        # Visualize
        output_path_person = os.path.join(output_dir, "dog_person_example_person.png")
        plot_results(image_pil, inference_state)
        plt.axis('off')
        plt.savefig(output_path_person, bbox_inches='tight', pad_inches=0)
        plt.close()
        print(f"Saved result to {output_path_person}")



if __name__ == "__main__":
    main()
