{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3b1c46-9f5c-41c1-9101-85db8709ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a0db5-7f04-4845-8b11-684fe6e9f7f2",
   "metadata": {},
   "source": [
    "# Video object segmentation with EfficientSAM3 Stage 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d0b3c-4207-442d-969c-aa1cbb8fd4ad",
   "metadata": {},
   "source": [
    "This notebook shows how to use **EfficientSAM3 Stage 2** for video object segmentation, following the SAM 2 API.\n",
    "\n",
    "**EfficientSAM3 Stage 2** uses:\n",
    "- SAM3 ViT-Huge backbone (frozen)\n",
    "- **Hybrid Memory Module** (Global + Spatial Perceivers) - replaces SAM3's dense memory bank\n",
    "- Efficient Attention mechanisms\n",
    "- >10× reduction in memory tokens while maintaining tracking quality\n",
    "\n",
    "This notebook follows the SAM 2 API for interactive video segmentation.\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/sam3/blob/main/notebooks/efficientsam3_for_sam2_video_task_example.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26616201-06df-435b-98fd-ad17c373bb4a",
   "metadata": {},
   "source": [
    "## Environment Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491a127-4c01-48f5-9dc5-f148a9417fdf",
   "metadata": {},
   "source": [
    "First install `sam3` in your environment using the [installation instructions](https://github.com/facebookresearch/sam3?tab=readme-ov-file#installation) in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f74c53be-aab1-46b9-8c0b-068b52ef5948",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d824a4b2-71f3-4da3-bfc7-3249625e6730",
   "metadata": {},
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"Torchvision version:\", torchvision.__version__)\n",
    "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install opencv-python matplotlib scikit-learn\n",
    "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/sam3.git'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6aa9d-487f-4207-b657-8cff0902343e",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3cae821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 3 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5318a85-5bf7-4880-b2b3-15e4db24d796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon7108528_msi_linux/miniconda3/envs/efficientsam3/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/mnt/e/side_projects/efficientsam3/sam3/sam3/backbones/repvit.py:276: UserWarning: Overwriting repvit_m0_9 in registry with sam3.backbones.repvit.repvit_m0_9. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/mnt/e/side_projects/efficientsam3/sam3/sam3/backbones/repvit.py:312: UserWarning: Overwriting repvit_m1_0 in registry with sam3.backbones.repvit.repvit_m1_0. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/mnt/e/side_projects/efficientsam3/sam3/sam3/backbones/repvit.py:349: UserWarning: Overwriting repvit_m1_1 in registry with sam3.backbones.repvit.repvit_m1_1. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/mnt/e/side_projects/efficientsam3/sam3/sam3/backbones/repvit.py:384: UserWarning: Overwriting repvit_m1_5 in registry with sam3.backbones.repvit.repvit_m1_5. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/mnt/e/side_projects/efficientsam3/sam3/sam3/backbones/repvit.py:438: UserWarning: Overwriting repvit_m2_3 in registry with sam3.backbones.repvit.repvit_m2_3. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/home/simon7108528_msi_linux/miniconda3/envs/efficientsam3/lib/python3.12/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/mnt/e/side_projects/efficientsam3/sam3/sam3/backbones/tiny_vit.py:649: UserWarning: Overwriting tiny_vit_5m_224 in registry with sam3.backbones.tiny_vit.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/mnt/e/side_projects/efficientsam3/sam3/sam3/backbones/tiny_vit.py:662: UserWarning: Overwriting tiny_vit_11m_224 in registry with sam3.backbones.tiny_vit.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/mnt/e/side_projects/efficientsam3/sam3/sam3/backbones/tiny_vit.py:675: UserWarning: Overwriting tiny_vit_21m_224 in registry with sam3.backbones.tiny_vit.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/mnt/e/side_projects/efficientsam3/sam3/sam3/backbones/tiny_vit.py:688: UserWarning: Overwriting tiny_vit_21m_384 in registry with sam3.backbones.tiny_vit.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "/mnt/e/side_projects/efficientsam3/sam3/sam3/backbones/tiny_vit.py:702: UserWarning: Overwriting tiny_vit_21m_512 in registry with sam3.backbones.tiny_vit.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import sam3\n",
    "import torch\n",
    "from PIL import Image\n",
    "from sam3.visualization_utils import show_box, show_mask, show_points\n",
    "\n",
    "# font size for axes titles\n",
    "plt.rcParams[\"axes.titlesize\"] = 12\n",
    "plt.rcParams[\"figure.titlesize\"] = 12\n",
    "\n",
    "sam3_root = os.path.join(os.path.dirname(sam3.__file__), \"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8e0779-751f-4224-9b04-ed0f0b406500",
   "metadata": {},
   "source": [
    "### Loading the EfficientSAM3 Stage 2 tracking predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5f3245e-b4d6-418b-a42a-a67e0b3b5aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EfficientSAM3 Stage 2 from: /mnt/e/side_projects/output/efficient_sam3_stage2.pt\n",
      "This model uses Hybrid Memory Module (>10× fewer memory tokens)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[32mINFO 2025-12-02 15:17:02,812 1294417 sam3_video_base.py: 124:\u001b[0m setting max_num_objects=10000 and num_obj_for_compile=16\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/e/side_projects/output/efficient_sam3_stage2.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading EfficientSAM3 Stage 2 from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mefficient_checkpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThis model uses Hybrid Memory Module (>10× fewer memory tokens)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m sam3_model = \u001b[43mbuild_sam3_video_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mefficient_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m predictor = sam3_model.tracker\n\u001b[32m     13\u001b[39m predictor.backbone = sam3_model.detector.backbone\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/e/side_projects/efficientsam3/sam3/sam3/model_builder.py:1111\u001b[39m, in \u001b[36mbuild_sam3_video_model\u001b[39m\u001b[34m(checkpoint_path, load_from_HF, bpe_path, has_presence_token, geo_encoder_use_img_cross_attn, strict_state_dict_loading, apply_temporal_disambiguation, device, compile)\u001b[39m\n\u001b[32m   1109\u001b[39m     checkpoint_path = download_ckpt_from_hf()\n\u001b[32m   1110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1111\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mg_pathmgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m   1112\u001b[39m         ckpt = torch.load(f, map_location=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m, weights_only=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ckpt \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ckpt[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/efficientsam3/lib/python3.12/site-packages/iopath/common/file_io.py:1062\u001b[39m, in \u001b[36mPathManager.open\u001b[39m\u001b[34m(self, path, mode, buffering, **kwargs)\u001b[39m\n\u001b[32m   1059\u001b[39m \u001b[38;5;66;03m# pass enable mode to handler that will be logging\u001b[39;00m\n\u001b[32m   1060\u001b[39m \u001b[38;5;66;03m# read, write operations separately.\u001b[39;00m\n\u001b[32m   1061\u001b[39m handler.set_logging(\u001b[38;5;28mself\u001b[39m._enable_logging)\n\u001b[32m-> \u001b[39m\u001b[32m1062\u001b[39m bret = \u001b[43mhandler\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1064\u001b[39m kvs = \u001b[38;5;28mself\u001b[39m.__get_open_keys(path, mode, buffering)\n\u001b[32m   1065\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_tmetry_keys(handler, kvs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/efficientsam3/lib/python3.12/site-packages/iopath/common/file_io.py:645\u001b[39m, in \u001b[36mNativePathHandler._open\u001b[39m\u001b[34m(self, path, mode, buffering, encoding, errors, newline, closefd, opener, **kwargs)\u001b[39m\n\u001b[32m    605\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    606\u001b[39m \u001b[33;03mOpen a path.\u001b[39;00m\n\u001b[32m    607\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    642\u001b[39m \u001b[33;03m    file: a file-like object.\u001b[39;00m\n\u001b[32m    643\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    644\u001b[39m \u001b[38;5;28mself\u001b[39m._check_kwargs(kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m645\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    646\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_path_with_cwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopener\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopener\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/mnt/e/side_projects/output/efficient_sam3_stage2.pt'"
     ]
    }
   ],
   "source": [
    "from sam3.model_builder import build_sam3_video_model\n",
    "\n",
    "# Load EfficientSAM3 Stage 2 checkpoint\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(sam3_root, \"../..\"))\n",
    "efficient_checkpoint = os.path.join(project_root, \"output/efficient_sam3_stage2.pt\")\n",
    "\n",
    "print(f\"Loading EfficientSAM3 Stage 2 from: {efficient_checkpoint}\")\n",
    "print(\"This model uses Hybrid Memory Module (>10× fewer memory tokens)\")\n",
    "\n",
    "sam3_model = build_sam3_video_model(checkpoint_path=efficient_checkpoint)\n",
    "predictor = sam3_model.tracker\n",
    "predictor.backbone = sam3_model.detector.backbone\n",
    "\n",
    "print(\"✓ EfficientSAM3 Stage 2 loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff46b10-c17a-4a26-8004-8c6d80806b0a",
   "metadata": {},
   "source": [
    "#### Initialize the inference state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f594ac71-a6b9-461d-af27-500fa1d1a420",
   "metadata": {},
   "source": [
    "Just like SAM 2, SAM 3 requires stateful inference for interactive video segmentation, so we need to initialize an **inference state** on this video.\n",
    "\n",
    "During initialization, it loads all the JPEG frames in `video_path` and stores their pixels in `inference_state` (as shown in the progress bar below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baa05c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SAM3's example video (bedroom scene)\n",
    "video_path = f\"{sam3_root}/assets/videos/bedroom\"\n",
    "print(f\"Using video: {video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb1f3f6-d74d-4016-934c-8d2a14d1a543",
   "metadata": {},
   "source": [
    "### Example 1: Segment & track one object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d3127-67b2-45d2-9f32-8fe3e10dc5eb",
   "metadata": {},
   "source": [
    "Note: if you have run any previous tracking using this `inference_state`, please reset it first via `clear_all_points_in_video`.\n",
    "\n",
    "(The cell below is just for illustration; it's not needed to call `clear_all_points_in_video` here as this `inference_state` is just freshly initialized above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2646a1d-3401-438c-a653-55e0e56b7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.clear_all_points_in_video(inference_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aeb04d-8cba-4f57-95da-6e5a1796003e",
   "metadata": {},
   "source": [
    "#### Step 1: Add a first click on a frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695c7749-b523-4691-aad0-7558c5d1d68c",
   "metadata": {},
   "source": [
    "To get started, let's try to segment the child on the left.\n",
    "\n",
    "Here we make a **positive click** at (x, y) = (210, 350) with label `1`, by sending their coordinates and labels into the `add_new_points` API.\n",
    "\n",
    "Note: label `1` indicates a *positive click (to add a region)* while label `0` indicates a *negative click (to remove a region)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6778a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load video frames for visualization\n",
    "if video_path.endswith(\".mp4\"):\n",
    "    import cv2\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    video_frames_for_vis = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames_for_vis.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    cap.release()\n",
    "else:\n",
    "    # Load from image folder\n",
    "    video_frames_for_vis = glob.glob(os.path.join(video_path, \"*.jpg\"))\n",
    "    video_frames_for_vis.sort(key=lambda p: int(os.path.splitext(os.path.basename(p))[0]))\n",
    "    video_frames_for_vis = [np.array(Image.open(p)) for p in video_frames_for_vis]\n",
    "\n",
    "frame0 = video_frames_for_vis[0]\n",
    "width, height = frame0.shape[1], frame0.shape[0]\n",
    "print(f\"Loaded {len(video_frames_for_vis)} frames, size: {width}x{height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e749bab-0f36-4173-bf8d-0c20cd5214b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a positive click at (x, y) = (210, 350) to get started\n",
    "points = np.array([[210, 350]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1], np.int32)\n",
    "\n",
    "rel_points = [[x / width, y / height] for x, y in points]\n",
    "\n",
    "points_tensor = torch.tensor(rel_points, dtype=torch.float32)\n",
    "points_labels_tensor = torch.tensor(labels, dtype=torch.int32)\n",
    "\n",
    "_, out_obj_ids, low_res_masks, video_res_masks = predictor.add_new_points(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points_tensor,\n",
    "    labels=points_labels_tensor,\n",
    "    clear_old_points=False,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(frame0)\n",
    "show_points(points, labels, plt.gca())\n",
    "show_mask((video_res_masks[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89457875-93fa-40ed-b6dc-4e1c971a27f9",
   "metadata": {},
   "source": [
    "#### Step 2: Add a second click to refine the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75eb21b-1413-452c-827b-a04093c30c78",
   "metadata": {},
   "source": [
    "Hmm, it seems that although we wanted to segment the child on the left, the model predicts the mask for only the shorts -- this can happen since there is ambiguity from a single click about what the target object should be. We can refine the mask on this frame via another positive click on the child's shirt.\n",
    "\n",
    "Here we make a **second positive click** at (x, y) = (250, 220) with label `1` to expand the mask.\n",
    "\n",
    "Note: we need to send **all the clicks and their labels** (i.e. not just the last click) when calling `add_new_points`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ab3ec7-2537-4158-bf98-3d0977d8908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a 2nd positive click at (x, y) = (250, 220) to refine the mask\n",
    "# sending all clicks (and their labels) to `add_new_points_or_box`\n",
    "points = np.array([[210, 350], [250, 220]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1, 1], np.int32)\n",
    "\n",
    "rel_points = [[x / width, y / height] for x, y in points]\n",
    "\n",
    "points_tensor = torch.tensor(rel_points, dtype=torch.float32)\n",
    "points_labels_tensor = torch.tensor(labels, dtype=torch.int32)\n",
    "\n",
    "_, out_obj_ids, low_res_masks, video_res_masks  = predictor.add_new_points(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points_tensor,\n",
    "    labels=points_labels_tensor,\n",
    "    clear_old_points=False,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(frame0)\n",
    "show_points(points, labels, plt.gca())\n",
    "show_mask((video_res_masks[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4ab457-d91d-4ac8-b350-fbcd549fd3fd",
   "metadata": {},
   "source": [
    "With this 2nd refinement click, now we get a segmentation mask of the entire child on frame 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52015ac-1b7b-4c59-bca3-c2b28484cf46",
   "metadata": {},
   "source": [
    "#### Step 3: Propagate the prompts to get the masklet across the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b025bd-cd58-4bfb-9572-c8d2fd0a02ef",
   "metadata": {},
   "source": [
    "To get the masklet throughout the entire video, we propagate the prompts using the `propagate_in_video` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab45e932-b0d5-4983-9718-6ee77d1ac31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for frame_idx, obj_ids, low_res_masks, video_res_masks, obj_scores in predictor.propagate_in_video(inference_state, start_frame_idx=0, max_frame_num_to_track=240, reverse=False, propagate_preflight=True):\n",
    "    video_segments[frame_idx] = {\n",
    "        out_obj_id: (video_res_masks[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 30\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(video_frames_for_vis), vis_frame_stride):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"frame {out_frame_idx}\")\n",
    "    plt.imshow(video_frames_for_vis[out_frame_idx])\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e801b70-72df-4a72-b3fe-84f145e5e3f6",
   "metadata": {},
   "source": [
    "#### Step 4: Add new prompts to further refine the masklet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478958ab-29b4-4a75-bba4-adb1b03d0a2b",
   "metadata": {},
   "source": [
    "It appears that in the output masklet above, there are some small imperfections in boundary details on frame 150.\n",
    "\n",
    "With SAM 3 we can fix the model predictions interactively. We can add a **negative click** at (x, y) = (82, 415) on this frame with label `0` to refine the masklet. Here we call the `add_new_points_or_box` API with a different `frame_idx` argument to indicate the frame index we want to refine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a572ea9-5b7e-479c-b30c-93c38b121131",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 150  # further refine some details on this frame\n",
    "ann_obj_id = 1  # give a unique id to the object we interact with (it can be any integers)\n",
    "\n",
    "# show the segment before further refinement\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx} -- before refinement\")\n",
    "plt.imshow(video_frames_for_vis[ann_frame_idx])\n",
    "show_mask(video_segments[ann_frame_idx][ann_obj_id], plt.gca(), obj_id=ann_obj_id)\n",
    "\n",
    "# Let's add a negative click on this frame at (x, y) = (82, 415) to refine the segment\n",
    "points = np.array([[82, 410]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([0], np.int32)\n",
    "\n",
    "rel_points = [[x / width, y / height] for x, y in points]\n",
    "\n",
    "points_tensor = torch.tensor(rel_points, dtype=torch.float32)\n",
    "points_labels_tensor = torch.tensor(labels, dtype=torch.int32)\n",
    "\n",
    "_, out_obj_ids, low_res_masks, video_res_masks  = predictor.add_new_points(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points_tensor,\n",
    "    labels=points_labels_tensor,\n",
    "    clear_old_points=False,\n",
    ")\n",
    "\n",
    "\n",
    "# show the segment after the further refinement\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx} -- after refinement\")\n",
    "plt.imshow(video_frames_for_vis[ann_frame_idx])\n",
    "show_points(points, labels, plt.gca())\n",
    "show_mask((video_res_masks > 0.0).cpu().numpy(), plt.gca(), obj_id=ann_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a3950a-acf1-435c-bd64-94297267b5e9",
   "metadata": {},
   "source": [
    "#### Step 5: Propagate the prompts (again) to get the masklet across the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1954ecf-c2ec-4f9c-8d10-c4f527a10cd2",
   "metadata": {},
   "source": [
    "Let's get an updated masklet for the entire video. Here we call `propagate_in_video` again to propagate all the prompts after adding the new refinement click above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa96690-4a38-4a24-aa17-fd2f4db0e232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for frame_idx, obj_ids, low_res_masks, video_res_masks, obj_scores in predictor.propagate_in_video(inference_state, start_frame_idx=0, max_frame_num_to_track=300, reverse=False, propagate_preflight=True):\n",
    "    video_segments[frame_idx] = {\n",
    "        out_obj_id: (video_res_masks[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 30\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(video_frames_for_vis), vis_frame_stride):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"frame {out_frame_idx}\")\n",
    "    plt.imshow(video_frames_for_vis[out_frame_idx])\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607507e3-6a2b-4fd7-944c-2371bdab9d01",
   "metadata": {},
   "source": [
    "The segments now look good on all frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2502bb5a-3e1f-43d0-9f58-33f8676fff0d",
   "metadata": {},
   "source": [
    "### Example 2: Segment an object using box prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2d26c8-0432-48c6-997e-4a3b77bb5f6d",
   "metadata": {},
   "source": [
    "Note: if you have run any previous tracking using this `inference_state`, please reset it first via `clear_all_points_in_video`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe9183-abbb-4283-b0cb-d24f3d7beb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.clear_all_points_in_video(inference_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb6eae9-0f4c-434f-8089-a46c9ca59da5",
   "metadata": {},
   "source": [
    "In addition to using clicks as inputs, SAM 3 also supports segmenting and tracking objects in a video via **bounding boxes**.\n",
    "\n",
    "In the example below, we segment the child on the right using a **box prompt** of (x_min, y_min, x_max, y_max) = (300, 0, 500, 400) on frame 0 as input into the `add_new_points_or_box` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbfb273-4e14-495b-bd89-87a8baf52ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 4  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a box at (x_min, y_min, x_max, y_max) = (300, 0, 500, 400) to get started\n",
    "box = np.array([[300, 0, 500, 400]], dtype=np.float32)\n",
    "\n",
    "rel_box = [[xmin / width, ymin / height, xmax / width, ymax / height] for xmin, ymin, xmax, ymax in box]\n",
    "rel_box = np.array(rel_box, dtype=np.float32)\n",
    "\n",
    "_, out_obj_ids, low_res_masks, video_res_masks  = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    box=rel_box,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(video_frames_for_vis[ann_frame_idx])\n",
    "show_box(box[0], plt.gca())\n",
    "show_mask((video_res_masks[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=ann_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f9ba7-bf4d-47e5-9b02-8a424cab42cc",
   "metadata": {},
   "source": [
    "Here, SAM 3 gets a pretty good segmentation mask of the entire child, even though the input bounding box is not perfectly tight around the object.\n",
    "\n",
    "Similar to the previous example, if the returned mask from is not perfect when using a box prompt, we can also further **refine** the output using positive or negative clicks. To illustrate this, here we make a **positive click** at (x, y) = (460, 60) with label `1` to expand the segment around the child's hair.\n",
    "\n",
    "Note: to refine the segmentation mask from a box prompt, we need to send **both the original box input and all subsequent refinement clicks and their labels** when calling `add_new_points_or_box`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54906315-ab4c-4088-b866-4c22134d5b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 4  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a positive click at (x, y) = (460, 60) to refine the mask\n",
    "points = np.array([[460, 60]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1], np.int32)\n",
    "# note that we also need to send the original box input along with\n",
    "# the new refinement click together into `add_new_points_or_box`\n",
    "box = np.array([[300, 0, 500, 400]], dtype=np.float32)\n",
    "\n",
    "rel_box = [[xmin / width, ymin / height, xmax / width, ymax / height] for xmin, ymin, xmax, ymax in box]\n",
    "rel_box = np.array(rel_box, dtype=np.float32)\n",
    "\n",
    "rel_points = [[x / width, y / height] for x, y in points]\n",
    "\n",
    "points_tensor = torch.tensor(rel_points, dtype=torch.float32)\n",
    "points_labels_tensor = torch.tensor(labels, dtype=torch.int32)\n",
    "\n",
    "_, out_obj_ids, low_res_masks, video_res_masks  = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points_tensor,\n",
    "    labels=points_labels_tensor,\n",
    "    box=rel_box,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(video_frames_for_vis[ann_frame_idx])\n",
    "show_box(box[0], plt.gca())\n",
    "show_points(points, labels, plt.gca())\n",
    "show_mask((video_res_masks[0][0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73128cd6-dbfa-49f7-8d79-1a8e19835f7f",
   "metadata": {},
   "source": [
    "Then, to get the masklet throughout the entire video, we propagate the prompts using the `propagate_in_video` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd90557-a0dc-442e-b091-9c74c831bef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for frame_idx, obj_ids, low_res_masks, video_res_masks, obj_scores in predictor.propagate_in_video(inference_state, start_frame_idx=0, max_frame_num_to_track=300, reverse=False, propagate_preflight=True):\n",
    "    video_segments[frame_idx] = {\n",
    "        out_obj_id: (video_res_masks[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 30\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(video_frames_for_vis), vis_frame_stride):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"frame {out_frame_idx}\")\n",
    "    plt.imshow(video_frames_for_vis[out_frame_idx])\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e023f91f-0cc5-4980-ae8e-a13c5749112b",
   "metadata": {},
   "source": [
    "Note that in addition to clicks or boxes, SAM 3 also supports directly using a **mask prompt** as input via the `add_new_mask` method in the `Sam3TrackerPredictor` class. This can be helpful in e.g. semi-supervised VOS evaluations (see [tools/vos_inference.py](https://github.com/facebookresearch/sam2/blob/main/tools/vos_inference.py) for an example)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da018be8-a4ae-4943-b1ff-702c2b89cb68",
   "metadata": {},
   "source": [
    "### Example 3: Segment multiple objects simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea6c04c-3072-4876-b394-879321a48c4a",
   "metadata": {},
   "source": [
    "Note: if you have run any previous tracking using this `inference_state`, please reset it first via `clear_all_points_in_video`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b874c8-9f39-42d3-a667-54a0bd696410",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.clear_all_points_in_video(inference_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f3f7e6-4821-468c-84e4-f3a0435c9149",
   "metadata": {},
   "source": [
    "#### Step 1: Add two objects on a frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95158714-86d7-48a9-8365-b213f97cc9ca",
   "metadata": {},
   "source": [
    "SAM 3 can also segment and track two or more objects at the same time. One way, of course, is to do them one by one. However, it would be more efficient to batch them together (e.g. so that we can share the image features between objects to reduce computation costs).\n",
    "\n",
    "This time, let's focus on object parts and segment **the shirts of both childen** in this video. Here we add prompts for these two objects and assign each of them a unique object id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22d896d-3cd5-4fa0-9230-f33e217035dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = {}  # hold all the clicks we add for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d9ac57-b14a-4237-828d-927e422c518b",
   "metadata": {},
   "source": [
    "Add the first object (the left child's shirt) with a **positive click** at (x, y) = (200, 300) on frame 0.\n",
    "\n",
    "We assign it to object id `2` (it can be arbitrary integers, and only needs to be unique for each object to track), which is passed to the `add_new_points_or_box` API to distinguish the object we are clicking upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13432fc-f467-44d8-adfe-3e0c488046b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 2  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a positive click at (x, y) = (200, 300) to get started on the first object\n",
    "points = np.array([[200, 300]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1], np.int32)\n",
    "prompts[ann_obj_id] = points, labels\n",
    "\n",
    "rel_points = [[x / width, y / height] for x, y in points]\n",
    "points_tensor = torch.tensor(rel_points, dtype=torch.float32)\n",
    "points_labels_tensor = torch.tensor(labels, dtype=torch.int32)\n",
    "\n",
    "_, out_obj_ids, low_res_masks, video_res_masks = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points_tensor,\n",
    "    labels=points_labels_tensor,\n",
    ")\n",
    "\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(video_frames_for_vis[ann_frame_idx])\n",
    "for i, out_obj_id in enumerate(out_obj_ids):\n",
    "    show_points(points, labels, plt.gca())\n",
    "    show_points(*prompts[out_obj_id], plt.gca())\n",
    "    show_mask((video_res_masks[i][0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbbd51b-e1e2-4c36-99ec-1d9a1b49b0cd",
   "metadata": {},
   "source": [
    "Hmm, this time we just want to select the child's shirt, but the model predicts the mask for the entire child. Let's refine the prediction with a **negative click** at (x, y) = (275, 175)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ecf61d-662b-4f98-ae62-46557b219842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the first object\n",
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 2  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a 2nd negative click at (x, y) = (275, 175) to refine the first object\n",
    "# sending all clicks (and their labels) to `add_new_points_or_box`\n",
    "points = np.array([[200, 300], [275, 175]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1, 0], np.int32)\n",
    "prompts[ann_obj_id] = points, labels\n",
    "\n",
    "rel_points = [[x / width, y / height] for x, y in points]\n",
    "points_tensor = torch.tensor(rel_points, dtype=torch.float32)\n",
    "points_labels_tensor = torch.tensor(labels, dtype=torch.int32)\n",
    "\n",
    "\n",
    "_, out_obj_ids, low_res_masks, video_res_masks  = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=rel_points,\n",
    "    labels=points_labels_tensor,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(video_frames_for_vis[ann_frame_idx])\n",
    "for i, out_obj_id in enumerate(out_obj_ids):\n",
    "    show_points(points, labels, plt.gca())\n",
    "    show_points(*prompts[out_obj_id], plt.gca())\n",
    "    show_mask((video_res_masks[i][0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194718c1-734d-446c-a3ef-361057de2f31",
   "metadata": {},
   "source": [
    "After the 2nd negative click, now we get the left child's shirt as our first object.\n",
    "\n",
    "Let's move on to the second object (the right child's shirt) with a positive click at (x, y) = (400, 150) on frame 0. Here we assign object id `3` to this second object (it can be arbitrary integers, and only needs to be unique for each object to track).\n",
    "\n",
    "Note: when there are multiple objects, the `add_new_points_or_box` API will return a list of masks for each object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca1bde-62a4-40e6-98e4-15606441e52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 3  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's now move on to the second object we want to track (giving it object id `3`)\n",
    "# with a positive click at (x, y) = (400, 150)\n",
    "points = np.array([[400, 150]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1], np.int32)\n",
    "prompts[ann_obj_id] = points, labels\n",
    "\n",
    "rel_points = [[x / width, y / height] for x, y in points]\n",
    "points_tensor = torch.tensor(rel_points, dtype=torch.float32)\n",
    "points_labels_tensor = torch.tensor(labels, dtype=torch.int32)\n",
    "\n",
    "\n",
    "# `add_new_points_or_box` returns masks for all objects added so far on this interacted frame\n",
    "_, out_obj_ids, low_res_masks, video_res_masks = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points_tensor,\n",
    "    labels=points_labels_tensor,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame on all objects\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(video_frames_for_vis[ann_frame_idx])\n",
    "for i, out_obj_id in enumerate(out_obj_ids):\n",
    "    show_points(points, labels, plt.gca())\n",
    "    show_points(*prompts[out_obj_id], plt.gca())\n",
    "    show_mask((video_res_masks[i][0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f7add8-d577-4597-ae2f-654b8c7b05e0",
   "metadata": {},
   "source": [
    "This time the model predicts the mask of the shirt we want to track in just one click. Nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448733b8-ea8b-4078-995f-b676c3b558ba",
   "metadata": {},
   "source": [
    "#### Step 2: Propagate the prompts to get masklets across the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bd73de-d669-41c8-b6ba-943883f0caa2",
   "metadata": {},
   "source": [
    "Now, we propagate the prompts for both objects to get their masklets throughout the video.\n",
    "\n",
    "Note: when there are multiple objects, the `propagate_in_video` API will return a list of masks for each object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17737191-d62b-4611-b2c6-6d0418a9ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for frame_idx, obj_ids, low_res_masks, video_res_masks, obj_scores in predictor.propagate_in_video(inference_state, start_frame_idx=0, max_frame_num_to_track=300, reverse=False, propagate_preflight=True):\n",
    "    video_segments[frame_idx] = {\n",
    "        out_obj_id: (video_res_masks[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 30\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(video_frames_for_vis), vis_frame_stride):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"frame {out_frame_idx}\")\n",
    "    plt.imshow(video_frames_for_vis[out_frame_idx])\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a0b9d7-c78f-432b-afb0-11f2ea5b652a",
   "metadata": {},
   "source": [
    "Looks like both children's shirts are well segmented in this video.\n",
    "\n",
    "Now you can try SAM 3 on your own videos and use cases! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "efficientsam3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
